format_table.go:591 按特定的形式显示结果:  5
<table>
<thead><tr><th>Setting</th><th>Type</th><th>Default</th><th>Description</th></tr></thead>
format_table.go:186 这里产生结果=>  [<code>changefeed.experimental_poll_interval</code> duration <code>1s</code> polling interval for the prototype changefeed implementation (WARNING: may compromise cluster stability or correctness; do not edit without supervision)]
<tbody>
<tr><td><code>changefeed.experimental_poll_interval</code></td><td>duration</td><td><code>1s</code></td><td>polling interval for the prototype changefeed implementation (WARNING: may compromise cluster stability or correctness; do not edit without supervision)</td></tr>
format_table.go:186 这里产生结果=>  [<code>changefeed.push.enabled</code> boolean <code>true</code> if set, changed are pushed instead of pulled. This requires the kv.rangefeed.enabled setting. See https://www.cockroachlabs.com/docs/v19.1/change-data-capture.html#enable-rangefeeds-to-reduce-latency]
<tr><td><code>changefeed.push.enabled</code></td><td>boolean</td><td><code>true</code></td><td>if set, changed are pushed instead of pulled. This requires the kv.rangefeed.enabled setting. See https://www.cockroachlabs.com/docs/v19.1/change-data-capture.html#enable-rangefeeds-to-reduce-latency</td></tr>
format_table.go:186 这里产生结果=>  [<code>cloudstorage.gs.default.key</code> string <code></code> if set, JSON key to use during Google Cloud Storage operations]
<tr><td><code>cloudstorage.gs.default.key</code></td><td>string</td><td><code></code></td><td>if set, JSON key to use during Google Cloud Storage operations</td></tr>
format_table.go:186 这里产生结果=>  [<code>cloudstorage.http.custom_ca</code> string <code></code> custom root CA (appended to system's default CAs) for verifying certificates when interacting with HTTPS storage]
<tr><td><code>cloudstorage.http.custom_ca</code></td><td>string</td><td><code></code></td><td>custom root CA (appended to system's default CAs) for verifying certificates when interacting with HTTPS storage</td></tr>
format_table.go:186 这里产生结果=>  [<code>cloudstorage.timeout</code> duration <code>10m0s</code> the timeout for import/export storage operations]
<tr><td><code>cloudstorage.timeout</code></td><td>duration</td><td><code>10m0s</code></td><td>the timeout for import/export storage operations</td></tr>
format_table.go:186 这里产生结果=>  [<code>cluster.organization</code> string <code></code> organization name]
<tr><td><code>cluster.organization</code></td><td>string</td><td><code></code></td><td>organization name</td></tr>
format_table.go:186 这里产生结果=>  [<code>cluster.preserve_downgrade_option</code> string <code></code> disable (automatic or manual) cluster version upgrade from the specified version until reset]
<tr><td><code>cluster.preserve_downgrade_option</code></td><td>string</td><td><code></code></td><td>disable (automatic or manual) cluster version upgrade from the specified version until reset</td></tr>
format_table.go:186 这里产生结果=>  [<code>compactor.enabled</code> boolean <code>true</code> when false, the system will reclaim space occupied by deleted data less aggressively]
<tr><td><code>compactor.enabled</code></td><td>boolean</td><td><code>true</code></td><td>when false, the system will reclaim space occupied by deleted data less aggressively</td></tr>
format_table.go:186 这里产生结果=>  [<code>compactor.max_record_age</code> duration <code>24h0m0s</code> discard suggestions not processed within this duration (WARNING: may compromise cluster stability or correctness; do not edit without supervision)]
<tr><td><code>compactor.max_record_age</code></td><td>duration</td><td><code>24h0m0s</code></td><td>discard suggestions not processed within this duration (WARNING: may compromise cluster stability or correctness; do not edit without supervision)</td></tr>
format_table.go:186 这里产生结果=>  [<code>compactor.min_interval</code> duration <code>15s</code> minimum time interval to wait before compacting (WARNING: may compromise cluster stability or correctness; do not edit without supervision)]
<tr><td><code>compactor.min_interval</code></td><td>duration</td><td><code>15s</code></td><td>minimum time interval to wait before compacting (WARNING: may compromise cluster stability or correctness; do not edit without supervision)</td></tr>
format_table.go:186 这里产生结果=>  [<code>compactor.threshold_available_fraction</code> float <code>0.1</code> consider suggestions for at least the given percentage of the available logical space (zero to disable) (WARNING: may compromise cluster stability or correctness; do not edit without supervision)]
<tr><td><code>compactor.threshold_available_fraction</code></td><td>float</td><td><code>0.1</code></td><td>consider suggestions for at least the given percentage of the available logical space (zero to disable) (WARNING: may compromise cluster stability or correctness; do not edit without supervision)</td></tr>
format_table.go:186 这里产生结果=>  [<code>compactor.threshold_bytes</code> byte size <code>256 MiB</code> minimum expected logical space reclamation required before considering an aggregated suggestion (WARNING: may compromise cluster stability or correctness; do not edit without supervision)]
<tr><td><code>compactor.threshold_bytes</code></td><td>byte size</td><td><code>256 MiB</code></td><td>minimum expected logical space reclamation required before considering an aggregated suggestion (WARNING: may compromise cluster stability or correctness; do not edit without supervision)</td></tr>
format_table.go:186 这里产生结果=>  [<code>compactor.threshold_used_fraction</code> float <code>0.1</code> consider suggestions for at least the given percentage of the used logical space (zero to disable) (WARNING: may compromise cluster stability or correctness; do not edit without supervision)]
<tr><td><code>compactor.threshold_used_fraction</code></td><td>float</td><td><code>0.1</code></td><td>consider suggestions for at least the given percentage of the used logical space (zero to disable) (WARNING: may compromise cluster stability or correctness; do not edit without supervision)</td></tr>
format_table.go:186 这里产生结果=>  [<code>debug.panic_on_failed_assertions</code> boolean <code>false</code> panic when an assertion fails rather than reporting]
<tr><td><code>debug.panic_on_failed_assertions</code></td><td>boolean</td><td><code>false</code></td><td>panic when an assertion fails rather than reporting</td></tr>
format_table.go:186 这里产生结果=>  [<code>diagnostics.forced_stat_reset.interval</code> duration <code>2h0m0s</code> interval after which pending diagnostics statistics should be discarded even if not reported]
<tr><td><code>diagnostics.forced_stat_reset.interval</code></td><td>duration</td><td><code>2h0m0s</code></td><td>interval after which pending diagnostics statistics should be discarded even if not reported</td></tr>
format_table.go:186 这里产生结果=>  [<code>diagnostics.reporting.enabled</code> boolean <code>true</code> enable reporting diagnostic metrics to cockroach labs]
<tr><td><code>diagnostics.reporting.enabled</code></td><td>boolean</td><td><code>true</code></td><td>enable reporting diagnostic metrics to cockroach labs</td></tr>
format_table.go:186 这里产生结果=>  [<code>diagnostics.reporting.interval</code> duration <code>1h0m0s</code> interval at which diagnostics data should be reported (should be shorter than diagnostics.forced_stat_reset.interval)]
<tr><td><code>diagnostics.reporting.interval</code></td><td>duration</td><td><code>1h0m0s</code></td><td>interval at which diagnostics data should be reported (should be shorter than diagnostics.forced_stat_reset.interval)</td></tr>
format_table.go:186 这里产生结果=>  [<code>diagnostics.reporting.send_crash_reports</code> boolean <code>true</code> send crash and panic reports]
<tr><td><code>diagnostics.reporting.send_crash_reports</code></td><td>boolean</td><td><code>true</code></td><td>send crash and panic reports</td></tr>
format_table.go:186 这里产生结果=>  [<code>external.graphite.endpoint</code> string <code></code> if nonempty, push server metrics to the Graphite or Carbon server at the specified host:port]
<tr><td><code>external.graphite.endpoint</code></td><td>string</td><td><code></code></td><td>if nonempty, push server metrics to the Graphite or Carbon server at the specified host:port</td></tr>
format_table.go:186 这里产生结果=>  [<code>external.graphite.interval</code> duration <code>10s</code> the interval at which metrics are pushed to Graphite (if enabled)]
<tr><td><code>external.graphite.interval</code></td><td>duration</td><td><code>10s</code></td><td>the interval at which metrics are pushed to Graphite (if enabled)</td></tr>
format_table.go:186 这里产生结果=>  [<code>jobs.registry.leniency</code> duration <code>1m0s</code> the amount of time to defer any attempts to reschedule a job]
<tr><td><code>jobs.registry.leniency</code></td><td>duration</td><td><code>1m0s</code></td><td>the amount of time to defer any attempts to reschedule a job</td></tr>
format_table.go:186 这里产生结果=>  [<code>jobs.retention_time</code> duration <code>336h0m0s</code> the amount of time to retain records for completed jobs before]
<tr><td><code>jobs.retention_time</code></td><td>duration</td><td><code>336h0m0s</code></td><td>the amount of time to retain records for completed jobs before</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.allocator.lease_rebalancing_aggressiveness</code> float <code>1</code> set greater than 1.0 to rebalance leases toward load more aggressively, or between 0 and 1.0 to be more conservative about rebalancing leases]
<tr><td><code>kv.allocator.lease_rebalancing_aggressiveness</code></td><td>float</td><td><code>1</code></td><td>set greater than 1.0 to rebalance leases toward load more aggressively, or between 0 and 1.0 to be more conservative about rebalancing leases</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.allocator.load_based_lease_rebalancing.enabled</code> boolean <code>true</code> set to enable rebalancing of range leases based on load and latency]
<tr><td><code>kv.allocator.load_based_lease_rebalancing.enabled</code></td><td>boolean</td><td><code>true</code></td><td>set to enable rebalancing of range leases based on load and latency</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.allocator.load_based_rebalancing</code> enumeration <code>2</code> whether to rebalance based on the distribution of QPS across stores [off = 0, leases = 1, leases and replicas = 2]]
<tr><td><code>kv.allocator.load_based_rebalancing</code></td><td>enumeration</td><td><code>2</code></td><td>whether to rebalance based on the distribution of QPS across stores [off = 0, leases = 1, leases and replicas = 2]</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.allocator.qps_rebalance_threshold</code> float <code>0.25</code> minimum fraction away from the mean a store's QPS (such as queries per second) can be before it is considered overfull or underfull]
<tr><td><code>kv.allocator.qps_rebalance_threshold</code></td><td>float</td><td><code>0.25</code></td><td>minimum fraction away from the mean a store's QPS (such as queries per second) can be before it is considered overfull or underfull</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.allocator.range_rebalance_threshold</code> float <code>0.05</code> minimum fraction away from the mean a store's range count can be before it is considered overfull or underfull]
<tr><td><code>kv.allocator.range_rebalance_threshold</code></td><td>float</td><td><code>0.05</code></td><td>minimum fraction away from the mean a store's range count can be before it is considered overfull or underfull</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.bulk_io_write.concurrent_addsstable_requests</code> integer <code>1</code> number of AddSSTable requests a store will handle concurrently before queuing]
<tr><td><code>kv.bulk_io_write.concurrent_addsstable_requests</code></td><td>integer</td><td><code>1</code></td><td>number of AddSSTable requests a store will handle concurrently before queuing</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.bulk_io_write.concurrent_export_requests</code> integer <code>3</code> number of export requests a store will handle concurrently before queuing]
<tr><td><code>kv.bulk_io_write.concurrent_export_requests</code></td><td>integer</td><td><code>3</code></td><td>number of export requests a store will handle concurrently before queuing</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.bulk_io_write.concurrent_import_requests</code> integer <code>1</code> number of import requests a store will handle concurrently before queuing]
<tr><td><code>kv.bulk_io_write.concurrent_import_requests</code></td><td>integer</td><td><code>1</code></td><td>number of import requests a store will handle concurrently before queuing</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.bulk_io_write.max_rate</code> byte size <code>8.0 EiB</code> the rate limit (bytes/sec) to use for writes to disk on behalf of bulk io ops]
<tr><td><code>kv.bulk_io_write.max_rate</code></td><td>byte size</td><td><code>8.0 EiB</code></td><td>the rate limit (bytes/sec) to use for writes to disk on behalf of bulk io ops</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.bulk_sst.sync_size</code> byte size <code>2.0 MiB</code> threshold after which non-Rocks SST writes must fsync (0 disables)]
<tr><td><code>kv.bulk_sst.sync_size</code></td><td>byte size</td><td><code>2.0 MiB</code></td><td>threshold after which non-Rocks SST writes must fsync (0 disables)</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.closed_timestamp.close_fraction</code> float <code>0.2</code> fraction of closed timestamp target duration specifying how frequently the closed timestamp is advanced]
<tr><td><code>kv.closed_timestamp.close_fraction</code></td><td>float</td><td><code>0.2</code></td><td>fraction of closed timestamp target duration specifying how frequently the closed timestamp is advanced</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.closed_timestamp.follower_reads_enabled</code> boolean <code>true</code> allow (all) replicas to serve consistent historical reads based on closed timestamp information]
<tr><td><code>kv.closed_timestamp.follower_reads_enabled</code></td><td>boolean</td><td><code>true</code></td><td>allow (all) replicas to serve consistent historical reads based on closed timestamp information</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.closed_timestamp.target_duration</code> duration <code>30s</code> if nonzero, attempt to provide closed timestamp notifications for timestamps trailing cluster time by approximately this duration]
<tr><td><code>kv.closed_timestamp.target_duration</code></td><td>duration</td><td><code>30s</code></td><td>if nonzero, attempt to provide closed timestamp notifications for timestamps trailing cluster time by approximately this duration</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.follower_read.target_multiple</code> float <code>3</code> if above 1, encourages the distsender to perform a read against the closest replica if a request is older than kv.closed_timestamp.target_duration * (1 + kv.closed_timestamp.close_fraction * this) less a clock uncertainty interval. This value also is used to create follower_timestamp(). (WARNING: may compromise cluster stability or correctness; do not edit without supervision)]
<tr><td><code>kv.follower_read.target_multiple</code></td><td>float</td><td><code>3</code></td><td>if above 1, encourages the distsender to perform a read against the closest replica if a request is older than kv.closed_timestamp.target_duration * (1 + kv.closed_timestamp.close_fraction * this) less a clock uncertainty interval. This value also is used to create follower_timestamp(). (WARNING: may compromise cluster stability or correctness; do not edit without supervision)</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.import.batch_size</code> byte size <code>32 MiB</code> the maximum size of the payload in an AddSSTable request (WARNING: may compromise cluster stability or correctness; do not edit without supervision)]
<tr><td><code>kv.import.batch_size</code></td><td>byte size</td><td><code>32 MiB</code></td><td>the maximum size of the payload in an AddSSTable request (WARNING: may compromise cluster stability or correctness; do not edit without supervision)</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.raft.command.max_size</code> byte size <code>64 MiB</code> maximum size of a raft command]
<tr><td><code>kv.raft.command.max_size</code></td><td>byte size</td><td><code>64 MiB</code></td><td>maximum size of a raft command</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.raft_log.disable_synchronization_unsafe</code> boolean <code>false</code> set to true to disable synchronization on Raft log writes to persistent storage. Setting to true risks data loss or data corruption on server crashes. The setting is meant for internal testing only and SHOULD NOT be used in production.]
<tr><td><code>kv.raft_log.disable_synchronization_unsafe</code></td><td>boolean</td><td><code>false</code></td><td>set to true to disable synchronization on Raft log writes to persistent storage. Setting to true risks data loss or data corruption on server crashes. The setting is meant for internal testing only and SHOULD NOT be used in production.</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.range.backpressure_range_size_multiplier</code> float <code>2</code> multiple of range_max_bytes that a range is allowed to grow to without splitting before writes to that range are blocked, or 0 to disable]
<tr><td><code>kv.range.backpressure_range_size_multiplier</code></td><td>float</td><td><code>2</code></td><td>multiple of range_max_bytes that a range is allowed to grow to without splitting before writes to that range are blocked, or 0 to disable</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.range_descriptor_cache.size</code> integer <code>1000000</code> maximum number of entries in the range descriptor and leaseholder caches]
<tr><td><code>kv.range_descriptor_cache.size</code></td><td>integer</td><td><code>1000000</code></td><td>maximum number of entries in the range descriptor and leaseholder caches</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.range_merge.queue_enabled</code> boolean <code>true</code> whether the automatic merge queue is enabled]
<tr><td><code>kv.range_merge.queue_enabled</code></td><td>boolean</td><td><code>true</code></td><td>whether the automatic merge queue is enabled</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.range_merge.queue_interval</code> duration <code>1s</code> how long the merge queue waits between processing replicas (WARNING: may compromise cluster stability or correctness; do not edit without supervision)]
<tr><td><code>kv.range_merge.queue_interval</code></td><td>duration</td><td><code>1s</code></td><td>how long the merge queue waits between processing replicas (WARNING: may compromise cluster stability or correctness; do not edit without supervision)</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.range_split.by_load_enabled</code> boolean <code>true</code> allow automatic splits of ranges based on where load is concentrated]
<tr><td><code>kv.range_split.by_load_enabled</code></td><td>boolean</td><td><code>true</code></td><td>allow automatic splits of ranges based on where load is concentrated</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.range_split.load_qps_threshold</code> integer <code>250</code> the QPS over which, the range becomes a candidate for load based splitting]
<tr><td><code>kv.range_split.load_qps_threshold</code></td><td>integer</td><td><code>250</code></td><td>the QPS over which, the range becomes a candidate for load based splitting</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.rangefeed.concurrent_catchup_iterators</code> integer <code>64</code> number of rangefeeds catchup iterators a store will allow concurrently before queueing]
<tr><td><code>kv.rangefeed.concurrent_catchup_iterators</code></td><td>integer</td><td><code>64</code></td><td>number of rangefeeds catchup iterators a store will allow concurrently before queueing</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.rangefeed.enabled</code> boolean <code>false</code> if set, rangefeed registration is enabled]
<tr><td><code>kv.rangefeed.enabled</code></td><td>boolean</td><td><code>false</code></td><td>if set, rangefeed registration is enabled</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.snapshot_rebalance.max_rate</code> byte size <code>8.0 MiB</code> the rate limit (bytes/sec) to use for rebalance and upreplication snapshots]
<tr><td><code>kv.snapshot_rebalance.max_rate</code></td><td>byte size</td><td><code>8.0 MiB</code></td><td>the rate limit (bytes/sec) to use for rebalance and upreplication snapshots</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.snapshot_recovery.max_rate</code> byte size <code>8.0 MiB</code> the rate limit (bytes/sec) to use for recovery snapshots]
<tr><td><code>kv.snapshot_recovery.max_rate</code></td><td>byte size</td><td><code>8.0 MiB</code></td><td>the rate limit (bytes/sec) to use for recovery snapshots</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.transaction.max_intents_bytes</code> integer <code>262144</code> maximum number of bytes used to track write intents in transactions]
<tr><td><code>kv.transaction.max_intents_bytes</code></td><td>integer</td><td><code>262144</code></td><td>maximum number of bytes used to track write intents in transactions</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.transaction.max_refresh_spans_bytes</code> integer <code>256000</code> maximum number of bytes used to track refresh spans in serializable transactions]
<tr><td><code>kv.transaction.max_refresh_spans_bytes</code></td><td>integer</td><td><code>256000</code></td><td>maximum number of bytes used to track refresh spans in serializable transactions</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.transaction.write_pipelining_enabled</code> boolean <code>true</code> if enabled, transactional writes are pipelined through Raft consensus]
<tr><td><code>kv.transaction.write_pipelining_enabled</code></td><td>boolean</td><td><code>true</code></td><td>if enabled, transactional writes are pipelined through Raft consensus</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.transaction.write_pipelining_max_batch_size</code> integer <code>128</code> if non-zero, defines that maximum size batch that will be pipelined through Raft consensus]
<tr><td><code>kv.transaction.write_pipelining_max_batch_size</code></td><td>integer</td><td><code>128</code></td><td>if non-zero, defines that maximum size batch that will be pipelined through Raft consensus</td></tr>
format_table.go:186 这里产生结果=>  [<code>kv.transaction.write_pipelining_max_outstanding_size</code> byte size <code>256 KiB</code> maximum number of bytes used to track in-flight pipelined writes before disabling pipelining]
<tr><td><code>kv.transaction.write_pipelining_max_outstanding_size</code></td><td>byte size</td><td><code>256 KiB</code></td><td>maximum number of bytes used to track in-flight pipelined writes before disabling pipelining</td></tr>
format_table.go:186 这里产生结果=>  [<code>rocksdb.min_wal_sync_interval</code> duration <code>0s</code> minimum duration between syncs of the RocksDB WAL]
<tr><td><code>rocksdb.min_wal_sync_interval</code></td><td>duration</td><td><code>0s</code></td><td>minimum duration between syncs of the RocksDB WAL</td></tr>
format_table.go:186 这里产生结果=>  [<code>schemachanger.backfiller.buffer_size</code> byte size <code>196 MiB</code> amount to buffer in memory during backfills]
<tr><td><code>schemachanger.backfiller.buffer_size</code></td><td>byte size</td><td><code>196 MiB</code></td><td>amount to buffer in memory during backfills</td></tr>
format_table.go:186 这里产生结果=>  [<code>schemachanger.backfiller.max_sst_size</code> byte size <code>16 MiB</code> target size for ingested files during backfills]
<tr><td><code>schemachanger.backfiller.max_sst_size</code></td><td>byte size</td><td><code>16 MiB</code></td><td>target size for ingested files during backfills</td></tr>
format_table.go:186 这里产生结果=>  [<code>schemachanger.bulk_index_backfill.batch_size</code> integer <code>5000</code> number of rows to process at a time during bulk index backfill]
<tr><td><code>schemachanger.bulk_index_backfill.batch_size</code></td><td>integer</td><td><code>5000</code></td><td>number of rows to process at a time during bulk index backfill</td></tr>
format_table.go:186 这里产生结果=>  [<code>schemachanger.bulk_index_backfill.enabled</code> boolean <code>true</code> backfill indexes in bulk via addsstable]
<tr><td><code>schemachanger.bulk_index_backfill.enabled</code></td><td>boolean</td><td><code>true</code></td><td>backfill indexes in bulk via addsstable</td></tr>
format_table.go:186 这里产生结果=>  [<code>schemachanger.lease.duration</code> duration <code>5m0s</code> the duration of a schema change lease]
<tr><td><code>schemachanger.lease.duration</code></td><td>duration</td><td><code>5m0s</code></td><td>the duration of a schema change lease</td></tr>
format_table.go:186 这里产生结果=>  [<code>schemachanger.lease.renew_fraction</code> float <code>0.5</code> the fraction of schemachanger.lease_duration remaining to trigger a renew of the lease]
<tr><td><code>schemachanger.lease.renew_fraction</code></td><td>float</td><td><code>0.5</code></td><td>the fraction of schemachanger.lease_duration remaining to trigger a renew of the lease</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.clock.forward_jump_check_enabled</code> boolean <code>false</code> if enabled, forward clock jumps > max_offset/2 will cause a panic]
<tr><td><code>server.clock.forward_jump_check_enabled</code></td><td>boolean</td><td><code>false</code></td><td>if enabled, forward clock jumps > max_offset/2 will cause a panic</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.clock.persist_upper_bound_interval</code> duration <code>0s</code> the interval between persisting the wall time upper bound of the clock. The clock does not generate a wall time greater than the persisted timestamp and will panic if it sees a wall time greater than this value. When cockroach starts, it waits for the wall time to catch-up till this persisted timestamp. This guarantees monotonic wall time across server restarts. Not setting this or setting a value of 0 disables this feature.]
<tr><td><code>server.clock.persist_upper_bound_interval</code></td><td>duration</td><td><code>0s</code></td><td>the interval between persisting the wall time upper bound of the clock. The clock does not generate a wall time greater than the persisted timestamp and will panic if it sees a wall time greater than this value. When cockroach starts, it waits for the wall time to catch-up till this persisted timestamp. This guarantees monotonic wall time across server restarts. Not setting this or setting a value of 0 disables this feature.</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.consistency_check.interval</code> duration <code>24h0m0s</code> the time between range consistency checks; set to 0 to disable consistency checking]
<tr><td><code>server.consistency_check.interval</code></td><td>duration</td><td><code>24h0m0s</code></td><td>the time between range consistency checks; set to 0 to disable consistency checking</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.declined_reservation_timeout</code> duration <code>1s</code> the amount of time to consider the store throttled for up-replication after a reservation was declined]
<tr><td><code>server.declined_reservation_timeout</code></td><td>duration</td><td><code>1s</code></td><td>the amount of time to consider the store throttled for up-replication after a reservation was declined</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.eventlog.ttl</code> duration <code>2160h0m0s</code> if nonzero, event log entries older than this duration are deleted every 10m0s. Should not be lowered below 24 hours.]
<tr><td><code>server.eventlog.ttl</code></td><td>duration</td><td><code>2160h0m0s</code></td><td>if nonzero, event log entries older than this duration are deleted every 10m0s. Should not be lowered below 24 hours.</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.failed_reservation_timeout</code> duration <code>5s</code> the amount of time to consider the store throttled for up-replication after a failed reservation call]
<tr><td><code>server.failed_reservation_timeout</code></td><td>duration</td><td><code>5s</code></td><td>the amount of time to consider the store throttled for up-replication after a failed reservation call</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.goroutine_dump.num_goroutines_threshold</code> integer <code>1000</code> a threshold beyond which if number of goroutines increases, then goroutine dump can be triggered]
<tr><td><code>server.goroutine_dump.num_goroutines_threshold</code></td><td>integer</td><td><code>1000</code></td><td>a threshold beyond which if number of goroutines increases, then goroutine dump can be triggered</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.goroutine_dump.total_dump_size_limit</code> byte size <code>500 MiB</code> total size of goroutine dumps to be kept. Dumps are GC'ed in the order of creation time. The latest dump is always kept even if its size exceeds the limit.]
<tr><td><code>server.goroutine_dump.total_dump_size_limit</code></td><td>byte size</td><td><code>500 MiB</code></td><td>total size of goroutine dumps to be kept. Dumps are GC'ed in the order of creation time. The latest dump is always kept even if its size exceeds the limit.</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.heap_profile.max_profiles</code> integer <code>5</code> maximum number of profiles to be kept. Profiles with lower score are GC'ed, but latest profile is always kept.]
<tr><td><code>server.heap_profile.max_profiles</code></td><td>integer</td><td><code>5</code></td><td>maximum number of profiles to be kept. Profiles with lower score are GC'ed, but latest profile is always kept.</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.heap_profile.system_memory_threshold_fraction</code> float <code>0.85</code> fraction of system memory beyond which if Rss increases, then heap profile is triggered]
<tr><td><code>server.heap_profile.system_memory_threshold_fraction</code></td><td>float</td><td><code>0.85</code></td><td>fraction of system memory beyond which if Rss increases, then heap profile is triggered</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.host_based_authentication.configuration</code> string <code></code> host-based authentication configuration to use during connection authentication]
<tr><td><code>server.host_based_authentication.configuration</code></td><td>string</td><td><code></code></td><td>host-based authentication configuration to use during connection authentication</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.rangelog.ttl</code> duration <code>720h0m0s</code> if nonzero, range log entries older than this duration are deleted every 10m0s. Should not be lowered below 24 hours.]
<tr><td><code>server.rangelog.ttl</code></td><td>duration</td><td><code>720h0m0s</code></td><td>if nonzero, range log entries older than this duration are deleted every 10m0s. Should not be lowered below 24 hours.</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.remote_debugging.mode</code> string <code>local</code> set to enable remote debugging, localhost-only or disable (any, local, off)]
<tr><td><code>server.remote_debugging.mode</code></td><td>string</td><td><code>local</code></td><td>set to enable remote debugging, localhost-only or disable (any, local, off)</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.shutdown.drain_wait</code> duration <code>0s</code> the amount of time a server waits in an unready state before proceeding with the rest of the shutdown process]
<tr><td><code>server.shutdown.drain_wait</code></td><td>duration</td><td><code>0s</code></td><td>the amount of time a server waits in an unready state before proceeding with the rest of the shutdown process</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.shutdown.query_wait</code> duration <code>10s</code> the server will wait for at least this amount of time for active queries to finish]
<tr><td><code>server.shutdown.query_wait</code></td><td>duration</td><td><code>10s</code></td><td>the server will wait for at least this amount of time for active queries to finish</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.time_until_store_dead</code> duration <code>5m0s</code> the time after which if there is no new gossiped information about a store, it is considered dead]
<tr><td><code>server.time_until_store_dead</code></td><td>duration</td><td><code>5m0s</code></td><td>the time after which if there is no new gossiped information about a store, it is considered dead</td></tr>
format_table.go:186 这里产生结果=>  [<code>server.web_session_timeout</code> duration <code>168h0m0s</code> the duration that a newly created web session will be valid]
<tr><td><code>server.web_session_timeout</code></td><td>duration</td><td><code>168h0m0s</code></td><td>the duration that a newly created web session will be valid</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.defaults.default_int_size</code> integer <code>8</code> the size, in bytes, of an INT type]
<tr><td><code>sql.defaults.default_int_size</code></td><td>integer</td><td><code>8</code></td><td>the size, in bytes, of an INT type</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.defaults.distsql</code> enumeration <code>1</code> default distributed SQL execution mode [off = 0, auto = 1, on = 2]]
<tr><td><code>sql.defaults.distsql</code></td><td>enumeration</td><td><code>1</code></td><td>default distributed SQL execution mode [off = 0, auto = 1, on = 2]</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.defaults.experimental_vectorize</code> enumeration <code>0</code> default experimental_vectorize mode [off = 0, on = 1, always = 2]]
<tr><td><code>sql.defaults.experimental_vectorize</code></td><td>enumeration</td><td><code>0</code></td><td>default experimental_vectorize mode [off = 0, on = 1, always = 2]</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.defaults.optimizer</code> enumeration <code>1</code> default cost-based optimizer mode [off = 0, on = 1, local = 2]]
<tr><td><code>sql.defaults.optimizer</code></td><td>enumeration</td><td><code>1</code></td><td>default cost-based optimizer mode [off = 0, on = 1, local = 2]</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.defaults.reorder_joins_limit</code> integer <code>4</code> default number of joins to reorder]
<tr><td><code>sql.defaults.reorder_joins_limit</code></td><td>integer</td><td><code>4</code></td><td>default number of joins to reorder</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.defaults.results_buffer.size</code> byte size <code>16 KiB</code> default size of the buffer that accumulates results for a statement or a batch of statements before they are sent to the client. This can be overridden on an individual connection with the 'results_buffer_size' parameter. Note that auto-retries generally only happen while no results have been delivered to the client, so reducing this size can increase the number of retriable errors a client receives. On the other hand, increasing the buffer size can increase the delay until the client receives the first result row. Updating the setting only affects new connections. Setting to 0 disables any buffering.]
<tr><td><code>sql.defaults.results_buffer.size</code></td><td>byte size</td><td><code>16 KiB</code></td><td>default size of the buffer that accumulates results for a statement or a batch of statements before they are sent to the client. This can be overridden on an individual connection with the 'results_buffer_size' parameter. Note that auto-retries generally only happen while no results have been delivered to the client, so reducing this size can increase the number of retriable errors a client receives. On the other hand, increasing the buffer size can increase the delay until the client receives the first result row. Updating the setting only affects new connections. Setting to 0 disables any buffering.</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.defaults.serial_normalization</code> enumeration <code>0</code> default handling of SERIAL in table definitions [rowid = 0, virtual_sequence = 1, sql_sequence = 2]]
<tr><td><code>sql.defaults.serial_normalization</code></td><td>enumeration</td><td><code>0</code></td><td>default handling of SERIAL in table definitions [rowid = 0, virtual_sequence = 1, sql_sequence = 2]</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.distsql.distribute_index_joins</code> boolean <code>true</code> if set, for index joins we instantiate a join reader on every node that has a stream; if not set, we use a single join reader]
<tr><td><code>sql.distsql.distribute_index_joins</code></td><td>boolean</td><td><code>true</code></td><td>if set, for index joins we instantiate a join reader on every node that has a stream; if not set, we use a single join reader</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.distsql.flow_stream_timeout</code> duration <code>10s</code> amount of time incoming streams wait for a flow to be set up before erroring out]
<tr><td><code>sql.distsql.flow_stream_timeout</code></td><td>duration</td><td><code>10s</code></td><td>amount of time incoming streams wait for a flow to be set up before erroring out</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.distsql.interleaved_joins.enabled</code> boolean <code>true</code> if set we plan interleaved table joins instead of merge joins when possible]
<tr><td><code>sql.distsql.interleaved_joins.enabled</code></td><td>boolean</td><td><code>true</code></td><td>if set we plan interleaved table joins instead of merge joins when possible</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.distsql.max_running_flows</code> integer <code>500</code> maximum number of concurrent flows that can be run on a node]
<tr><td><code>sql.distsql.max_running_flows</code></td><td>integer</td><td><code>500</code></td><td>maximum number of concurrent flows that can be run on a node</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.distsql.merge_joins.enabled</code> boolean <code>true</code> if set, we plan merge joins when possible]
<tr><td><code>sql.distsql.merge_joins.enabled</code></td><td>boolean</td><td><code>true</code></td><td>if set, we plan merge joins when possible</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.distsql.temp_storage.joins</code> boolean <code>true</code> set to true to enable use of disk for distributed sql joins]
<tr><td><code>sql.distsql.temp_storage.joins</code></td><td>boolean</td><td><code>true</code></td><td>set to true to enable use of disk for distributed sql joins</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.distsql.temp_storage.sorts</code> boolean <code>true</code> set to true to enable use of disk for distributed sql sorts]
<tr><td><code>sql.distsql.temp_storage.sorts</code></td><td>boolean</td><td><code>true</code></td><td>set to true to enable use of disk for distributed sql sorts</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.distsql.temp_storage.workmem</code> byte size <code>64 MiB</code> maximum amount of memory in bytes a processor can use before falling back to temp storage]
<tr><td><code>sql.distsql.temp_storage.workmem</code></td><td>byte size</td><td><code>64 MiB</code></td><td>maximum amount of memory in bytes a processor can use before falling back to temp storage</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.metrics.statement_details.dump_to_logs</code> boolean <code>false</code> dump collected statement statistics to node logs when periodically cleared]
<tr><td><code>sql.metrics.statement_details.dump_to_logs</code></td><td>boolean</td><td><code>false</code></td><td>dump collected statement statistics to node logs when periodically cleared</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.metrics.statement_details.enabled</code> boolean <code>true</code> collect per-statement query statistics]
<tr><td><code>sql.metrics.statement_details.enabled</code></td><td>boolean</td><td><code>true</code></td><td>collect per-statement query statistics</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.metrics.statement_details.plan_collection.enabled</code> boolean <code>true</code> periodically save a logical plan for each fingerprint]
<tr><td><code>sql.metrics.statement_details.plan_collection.enabled</code></td><td>boolean</td><td><code>true</code></td><td>periodically save a logical plan for each fingerprint</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.metrics.statement_details.plan_collection.period</code> duration <code>5m0s</code> the time until a new logical plan is collected]
<tr><td><code>sql.metrics.statement_details.plan_collection.period</code></td><td>duration</td><td><code>5m0s</code></td><td>the time until a new logical plan is collected</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.metrics.statement_details.threshold</code> duration <code>0s</code> minimum execution time to cause statistics to be collected]
<tr><td><code>sql.metrics.statement_details.threshold</code></td><td>duration</td><td><code>0s</code></td><td>minimum execution time to cause statistics to be collected</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.parallel_scans.enabled</code> boolean <code>true</code> parallelizes scanning different ranges when the maximum result size can be deduced]
<tr><td><code>sql.parallel_scans.enabled</code></td><td>boolean</td><td><code>true</code></td><td>parallelizes scanning different ranges when the maximum result size can be deduced</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.query_cache.enabled</code> boolean <code>true</code> enable the query cache]
<tr><td><code>sql.query_cache.enabled</code></td><td>boolean</td><td><code>true</code></td><td>enable the query cache</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.stats.automatic_collection.enabled</code> boolean <code>true</code> automatic statistics collection mode]
<tr><td><code>sql.stats.automatic_collection.enabled</code></td><td>boolean</td><td><code>true</code></td><td>automatic statistics collection mode</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.stats.automatic_collection.fraction_stale_rows</code> float <code>0.2</code> target fraction of stale rows per table that will trigger a statistics refresh]
<tr><td><code>sql.stats.automatic_collection.fraction_stale_rows</code></td><td>float</td><td><code>0.2</code></td><td>target fraction of stale rows per table that will trigger a statistics refresh</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.stats.automatic_collection.max_fraction_idle</code> float <code>0.9</code> maximum fraction of time that automatic statistics sampler processors are idle]
<tr><td><code>sql.stats.automatic_collection.max_fraction_idle</code></td><td>float</td><td><code>0.9</code></td><td>maximum fraction of time that automatic statistics sampler processors are idle</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.stats.automatic_collection.min_stale_rows</code> integer <code>500</code> target minimum number of stale rows per table that will trigger a statistics refresh]
<tr><td><code>sql.stats.automatic_collection.min_stale_rows</code></td><td>integer</td><td><code>500</code></td><td>target minimum number of stale rows per table that will trigger a statistics refresh</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.stats.post_events.enabled</code> boolean <code>false</code> if set, an event is shown for every CREATE STATISTICS job]
<tr><td><code>sql.stats.post_events.enabled</code></td><td>boolean</td><td><code>false</code></td><td>if set, an event is shown for every CREATE STATISTICS job</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.tablecache.lease.refresh_limit</code> integer <code>50</code> maximum number of tables to periodically refresh leases for]
<tr><td><code>sql.tablecache.lease.refresh_limit</code></td><td>integer</td><td><code>50</code></td><td>maximum number of tables to periodically refresh leases for</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.trace.log_statement_execute</code> boolean <code>false</code> set to true to enable logging of executed statements]
<tr><td><code>sql.trace.log_statement_execute</code></td><td>boolean</td><td><code>false</code></td><td>set to true to enable logging of executed statements</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.trace.session_eventlog.enabled</code> boolean <code>false</code> set to true to enable session tracing]
<tr><td><code>sql.trace.session_eventlog.enabled</code></td><td>boolean</td><td><code>false</code></td><td>set to true to enable session tracing</td></tr>
format_table.go:186 这里产生结果=>  [<code>sql.trace.txn.enable_threshold</code> duration <code>0s</code> duration beyond which all transactions are traced (set to 0 to disable)]
<tr><td><code>sql.trace.txn.enable_threshold</code></td><td>duration</td><td><code>0s</code></td><td>duration beyond which all transactions are traced (set to 0 to disable)</td></tr>
format_table.go:186 这里产生结果=>  [<code>timeseries.storage.enabled</code> boolean <code>true</code> if set, periodic timeseries data is stored within the cluster; disabling is not recommended unless you are storing the data elsewhere]
<tr><td><code>timeseries.storage.enabled</code></td><td>boolean</td><td><code>true</code></td><td>if set, periodic timeseries data is stored within the cluster; disabling is not recommended unless you are storing the data elsewhere</td></tr>
format_table.go:186 这里产生结果=>  [<code>timeseries.storage.resolution_10s.ttl</code> duration <code>240h0m0s</code> the maximum age of time series data stored at the 10 second resolution. Data older than this is subject to rollup and deletion.]
<tr><td><code>timeseries.storage.resolution_10s.ttl</code></td><td>duration</td><td><code>240h0m0s</code></td><td>the maximum age of time series data stored at the 10 second resolution. Data older than this is subject to rollup and deletion.</td></tr>
format_table.go:186 这里产生结果=>  [<code>timeseries.storage.resolution_30m.ttl</code> duration <code>2160h0m0s</code> the maximum age of time series data stored at the 30 minute resolution. Data older than this is subject to deletion.]
<tr><td><code>timeseries.storage.resolution_30m.ttl</code></td><td>duration</td><td><code>2160h0m0s</code></td><td>the maximum age of time series data stored at the 30 minute resolution. Data older than this is subject to deletion.</td></tr>
format_table.go:186 这里产生结果=>  [<code>trace.debug.enable</code> boolean <code>false</code> if set, traces for recent requests can be seen in the /debug page]
<tr><td><code>trace.debug.enable</code></td><td>boolean</td><td><code>false</code></td><td>if set, traces for recent requests can be seen in the /debug page</td></tr>
format_table.go:186 这里产生结果=>  [<code>trace.lightstep.token</code> string <code></code> if set, traces go to Lightstep using this token]
<tr><td><code>trace.lightstep.token</code></td><td>string</td><td><code></code></td><td>if set, traces go to Lightstep using this token</td></tr>
format_table.go:186 这里产生结果=>  [<code>trace.zipkin.collector</code> string <code></code> if set, traces go to the given Zipkin instance (example: '127.0.0.1:9411'); ignored if trace.lightstep.token is set]
<tr><td><code>trace.zipkin.collector</code></td><td>string</td><td><code></code></td><td>if set, traces go to the given Zipkin instance (example: '127.0.0.1:9411'); ignored if trace.lightstep.token is set</td></tr>
format_table.go:186 这里产生结果=>  [<code>version</code> custom validation <code>19.1</code> set the active cluster version in the format '<major>.<minor>']
<tr><td><code>version</code></td><td>custom validation</td><td><code>19.1</code></td><td>set the active cluster version in the format '<major>.<minor>'</td></tr>
format_table.go:186 这里产生结果=>  []
format_table.go:223 render...，得看看谁实现了 rowReporter
</tbody>
</table>
